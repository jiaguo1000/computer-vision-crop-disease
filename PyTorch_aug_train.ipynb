{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from skimage.util import img_as_float\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from early_stopping_class import EarlyStopping\n",
    "from data_loader import *\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "__C = edict()\n",
    "cfg = __C\n",
    "__C.CUDA = torch.cuda.is_available()\n",
    "__C.WORKERS = 4\n",
    "__C.MAX_EPOCH = 100\n",
    "__C.MIN_EPOCH = 5\n",
    "__C.BATCH_SIZE = 32\n",
    "__C.VERBOSE = True\n",
    "\n",
    "# Fix the random seed\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0) \n",
    "\n",
    "# Check device, using gpu 0 if gpu exist else using cpu\n",
    "device = torch.device(\"cuda:0\" if cfg.CUDA else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/'\n",
    "\n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir,\n",
    "                                                    batch_size=32,\n",
    "                                                    image_size=(224, 224),\n",
    "                                                    augment=False,\n",
    "                                                    random_seed=1993,\n",
    "                                                    valid_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    show_sample=True,\n",
    "                                                    num_workers=cfg.WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, train_generator, dev_generator, patience, n_epochs):\n",
    "    \"\"\"\n",
    "    Perform the actual training of the model based on the train and dev sets.\n",
    "    :param model: one of your models, to be trained to perform 4-way emotion classification\n",
    "    :param loss_fn: a function that can calculate loss between the predicted and gold labels\n",
    "    :param optimizer: a created optimizer you will use to update your model weights\n",
    "    :param train_generator: a DataLoader that provides batches of the training set\n",
    "    :param dev_generator: a DataLoader that provides batches of the development set\n",
    "    :return model, the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    valloss = []\n",
    "    valacc = []\n",
    "    time0 = time.time()\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=cfg.VERBOSE)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        epoch_loss = 0\n",
    "        train_acc = 0\n",
    "        j = 0\n",
    "        model.train()\n",
    "        for X_b, y_b in train_generator:\n",
    "            j+=1\n",
    "            num_b = len(y_b)\n",
    "            X_b = X_b.float().to(device)          \n",
    "            y_b = y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_b)\n",
    "            train_loss = loss_fn(y_pred, y_b.long())\n",
    "            _,indices = y_pred.max(1)\n",
    "            correct = (indices == y_b).float() #convert into float for division \n",
    "            train_acc += correct.sum() / len(correct)\n",
    "            epoch_loss += train_loss\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss = epoch_loss/len(train_generator)\n",
    "        train_acc = train_acc/len(train_generator)\n",
    "\n",
    "    # on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_acc = 0\n",
    "            valid_loss = 0\n",
    "            for X_b, y_b in dev_generator:\n",
    "                # Predict\n",
    "                X_b = X_b.float().to(device)\n",
    "                y_b = y_b.to(device)\n",
    "                y_pred = model(X_b)\n",
    "                valid_loss += loss_fn(y_pred, y_b.long()).item()\n",
    "                _,indices = y_pred.max(1)\n",
    "                correct = (indices == y_b).float()\n",
    "                valid_acc += correct.sum() / len(correct)\n",
    "            valid_loss = valid_loss/len(dev_generator)\n",
    "            valid_acc = valid_acc/len(dev_generator)\n",
    "            end = time.time()\n",
    "            \n",
    "            if cfg.VERBOSE:\n",
    "                print(f'Epoch: {epoch+1:02}')\n",
    "                print(f'\\tTrain Loss: {epoch_loss:.3f}| Train Acc: {train_acc*100:.2f}% | Time: {end-start:.2f}s')\n",
    "                print(f'\\t Val. Loss: {valid_loss:.3f}|  Val. Acc: {valid_acc*100:.2f}%')\n",
    "            \n",
    "            valacc.append(valid_acc)\n",
    "            valloss.append(valid_loss)\n",
    "            \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    print('Total time = %.2fs'%(end - time0))\n",
    "    return model,valloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.drop1 = nn.Dropout2d(p=0.2)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = x.view(-1, 16 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "# model_ft = models.alexnet(pretrained=True)\n",
    "# set_parameter_requires_grad(model_ft, feature_extract)\n",
    "# num_ftrs = model_ft.classifier[6].in_features\n",
    "# model_ft.classifier[6] = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "# model_ft = models.resnet18(pretrained=True)\n",
    "# set_parameter_requires_grad(model_ft, feature_extract)\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_ft = models.vgg16_bn(pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extract)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, 512)\n",
    "\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(model_ft,\n",
    "                      nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(inplace=True),\n",
    "                      nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "                      nn.Linear(128, 3), nn.Softmax())\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        model.to(device)\n",
    "        \n",
    "dmodel,_ = train_model(model, loss_fn, optimizer, train_loader, valid_loader, patience=10, n_epochs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/jupyter/test_224/'\n",
    "\n",
    "test_loader = get_test_loader(data_dir,\n",
    "                            batch_size=32,\n",
    "                            image_size=(224,224),\n",
    "                            shuffle=True,\n",
    "                            show_sample=True,\n",
    "                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "img_name = []\n",
    "\n",
    "dmodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for temp_data in test_loader:\n",
    "        # Predict\n",
    "        X_b = temp_data['image'].float().to(device)\n",
    "        x_name = temp_data['name']\n",
    "        y_pred = dmodel(X_b)\n",
    "#         y_pred = F.softmax(y_pred)\n",
    "        \n",
    "        predicted.extend(y_pred.cpu().detach().numpy())\n",
    "        img_name.extend(x_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_wheat = pd.Series(range(610), name=\"healthy_wheat\", dtype=np.float32)\n",
    "leaf_rust = pd.Series(range(610), name=\"leaf_rust\", dtype=np.float32)\n",
    "stem_rust = pd.Series(range(610), name=\"stem_rust\", dtype=np.float32)\n",
    "\n",
    "sub = pd.concat([healthy_wheat, leaf_rust, stem_rust], axis=1)\n",
    "\n",
    "# append real predictions to the dataset\n",
    "for i in tqdm(range(0 ,len(predicted))):\n",
    "    sub.loc[i] = predicted[i]\n",
    "\n",
    "sub[\"ID\"] = img_name\n",
    "cols = sub.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "sub = sub[cols]\n",
    "sub.to_csv(\"sub.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
